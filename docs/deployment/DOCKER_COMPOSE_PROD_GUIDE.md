# Docker Compose Production Guide

## Overview

The `docker-compose.prod.yml` file is automatically generated by the CI/CD pipeline and contains all services configured to use published Docker images from GitHub Container Registry.

## How It Works

### Automatic Generation

1. **When:** Every time code is pushed to `main`/`master` branch
2. **What:** GitHub Actions workflow builds images and generates `docker-compose.prod.yml`
3. **Where:** File is committed back to the repository
4. **Version:** Uses the auto-incremented version tag (e.g., `v0.0.2`)

### File Structure

```yaml
version: '3.8'

services:
  # Infrastructure (always running)
  - zookeeper
  - kafka
  - postgres
  - redis
  
  # Application (uses published images)
  - api
  - consumer
  - bridge
  - ui
```

## Services Included

### Infrastructure Services

#### Zookeeper
- **Image:** `confluentinc/cp-zookeeper:7.5.0`
- **Port:** 2181
- **Purpose:** Kafka coordination
- **Resources:** Minimal

#### Kafka
- **Image:** `confluentinc/cp-kafka:7.5.0`
- **Port:** 9092
- **Purpose:** Message broker
- **Resources:** 2 CPU, 4 GB RAM
- **Retention:** 7 days, 1 GB per partition

#### PostgreSQL
- **Image:** `postgres:16-alpine`
- **Port:** 5432
- **Purpose:** Workflow state database
- **Resources:** 2 CPU, 4 GB RAM
- **Health Check:** `pg_isready`
- **Volume:** `postgres-data`

#### Redis
- **Image:** `redis:7-alpine`
- **Port:** 6379
- **Purpose:** Circuit breaker state, caching
- **Resources:** 1 CPU, 2 GB RAM
- **Health Check:** `redis-cli ping`
- **Volume:** `redis-data`
- **Persistence:** AOF enabled

### Application Services

#### API
- **Image:** `ghcr.io/{owner}/orchestrator-api:{version}`
- **Port:** 8000
- **Purpose:** REST API
- **Resources:** 2 CPU, 2 GB RAM
- **Health Check:** HTTP GET `/health`
- **Depends On:** postgres (healthy), kafka (started)

#### Consumer
- **Image:** `ghcr.io/{owner}/orchestrator-consumer:{version}`
- **Purpose:** Workflow execution consumer
- **Resources:** 2 CPU, 2 GB RAM
- **Depends On:** postgres (healthy), kafka (started)

#### Bridge
- **Image:** `ghcr.io/{owner}/orchestrator-bridge:{version}`
- **Purpose:** HTTP-Kafka bridge for agent communication
- **Resources:** 2 CPU, 2 GB RAM
- **Depends On:** kafka (started), redis (healthy)

#### UI
- **Image:** `ghcr.io/{owner}/orchestrator-ui:{version}`
- **Port:** 3000
- **Purpose:** Web interface
- **Resources:** 1 CPU, 512 MB RAM
- **Health Check:** HTTP GET `/`
- **Depends On:** api (healthy)

## Resource Allocation

### Total Resources (Limits)

```
Service      CPU    RAM     Notes
─────────────────────────────────────────
Zookeeper    -      -       Minimal
Kafka        2      4 GB    Message broker
PostgreSQL   2      4 GB    Database
Redis        1      2 GB    Cache
API          2      2 GB    REST API
Consumer     2      2 GB    Workflow consumer
Bridge       2      2 GB    HTTP bridge
UI           1      512 MB  Web interface
─────────────────────────────────────────
Total        12     16.5 GB (+ overhead)
```

**Recommended Machine:** 16 CPU, 32 GB RAM

### Resource Reservations

Each service has minimum resource reservations to ensure stable operation:
- API: 0.5 CPU, 512 MB RAM
- Consumer: 0.5 CPU, 512 MB RAM
- Bridge: 0.5 CPU, 512 MB RAM
- UI: 0.25 CPU, 128 MB RAM

## Deployment

### Prerequisites

1. **Docker & Docker Compose installed**
   ```bash
   docker --version
   docker-compose --version
   ```

2. **Environment files configured**
   ```bash
   cp .env.example .env
   cp .env.api.example .env.api
   ```

3. **GitHub Container Registry access** (for private images)
   ```bash
   echo $GITHUB_TOKEN | docker login ghcr.io -u USERNAME --password-stdin
   ```

### Quick Start

```bash
# Pull the latest docker-compose.prod.yml
git pull origin main

# Start all services
docker-compose -f docker-compose.prod.yml up -d

# Check status
docker-compose -f docker-compose.prod.yml ps

# View logs
docker-compose -f docker-compose.prod.yml logs -f
```

### Step-by-Step Deployment

#### 1. Pull Images

```bash
# Pull all images (optional, docker-compose will do this automatically)
docker-compose -f docker-compose.prod.yml pull
```

#### 2. Start Infrastructure

```bash
# Start infrastructure services first
docker-compose -f docker-compose.prod.yml up -d zookeeper kafka postgres redis

# Wait for services to be healthy
docker-compose -f docker-compose.prod.yml ps
```

#### 3. Run Migrations

```bash
# Run database migrations
docker-compose -f docker-compose.prod.yml run --rm api alembic upgrade head
```

#### 4. Start Application Services

```bash
# Start application services
docker-compose -f docker-compose.prod.yml up -d api consumer bridge ui
```

#### 5. Verify Deployment

```bash
# Check all services are running
docker-compose -f docker-compose.prod.yml ps

# Check health
curl http://localhost:8000/health
curl http://localhost:3000/

# View logs
docker-compose -f docker-compose.prod.yml logs -f api
```

## Health Checks

### Service Health Status

```bash
# Check all health checks
docker-compose -f docker-compose.prod.yml ps

# Services with health checks:
# - postgres: pg_isready
# - redis: redis-cli ping
# - api: HTTP GET /health
# - ui: HTTP GET /
```

### Dependency Management

Services use `depends_on` with conditions:
- `service_healthy`: Waits for health check to pass
- `service_started`: Waits for service to start (no health check)

This ensures proper startup order:
1. Zookeeper starts
2. Kafka starts (depends on Zookeeper)
3. PostgreSQL starts and becomes healthy
4. Redis starts and becomes healthy
5. API starts (depends on healthy PostgreSQL and started Kafka)
6. Consumer starts (depends on healthy PostgreSQL and started Kafka)
7. Bridge starts (depends on started Kafka and healthy Redis)
8. UI starts (depends on healthy API)

## Configuration

### Environment Variables

#### Required in `.env`
```bash
# Kafka
KAFKA_BROKERS=kafka:29092

# Database
DATABASE_URL=postgresql://workflow_user:workflow_pass@postgres:5432/workflow_db

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json
```

#### Required in `.env.api`
```bash
# API Configuration
API_HOST=0.0.0.0
API_PORT=8000

# Database
DATABASE_URL=postgresql://workflow_user:workflow_pass@postgres:5432/workflow_db

# Kafka
KAFKA_BROKERS=kafka:29092
```

#### Optional Environment Variables
```bash
# UI Configuration
VITE_API_URL=http://localhost:8000/api/v1
VITE_POLLING_INTERVAL=2000
VITE_APP_NAME=Workflow Manager

# PostgreSQL (override defaults)
POSTGRES_USER=workflow_user
POSTGRES_PASSWORD=CHANGE_IN_PRODUCTION
POSTGRES_DB=workflow_db
```

### Volumes

#### Persistent Data

```yaml
volumes:
  postgres-data:    # Database data
  redis-data:       # Redis persistence
```

**Location:** `/var/lib/docker/volumes/`

**Backup:**
```bash
# Backup PostgreSQL
docker-compose -f docker-compose.prod.yml exec postgres pg_dump -U workflow_user workflow_db > backup.sql

# Backup Redis
docker-compose -f docker-compose.prod.yml exec redis redis-cli SAVE
```

## Monitoring

### Resource Usage

```bash
# Monitor resource usage
docker stats

# Specific service
docker stats workflow-management-api
```

### Logs

```bash
# All services
docker-compose -f docker-compose.prod.yml logs -f

# Specific service
docker-compose -f docker-compose.prod.yml logs -f api

# Last 100 lines
docker-compose -f docker-compose.prod.yml logs --tail=100 api

# Follow with timestamps
docker-compose -f docker-compose.prod.yml logs -f -t api
```

### Health Checks

```bash
# API health
curl http://localhost:8000/health

# API readiness
curl http://localhost:8000/health/ready

# UI health
curl http://localhost:3000/

# Database
docker-compose -f docker-compose.prod.yml exec postgres pg_isready -U workflow_user

# Redis
docker-compose -f docker-compose.prod.yml exec redis redis-cli ping
```

## Scaling

### Horizontal Scaling

Scale services that support it:

```bash
# Scale API (multiple instances)
docker-compose -f docker-compose.prod.yml up -d --scale api=3

# Scale Consumer (Kafka consumer group)
docker-compose -f docker-compose.prod.yml up -d --scale consumer=2

# Scale Bridge
docker-compose -f docker-compose.prod.yml up -d --scale bridge=2
```

**Note:** You'll need a load balancer for API scaling.

### Vertical Scaling

Edit resource limits in `docker-compose.prod.yml`:

```yaml
deploy:
  resources:
    limits:
      cpus: '4'      # Increase CPU
      memory: 4G     # Increase RAM
```

## Updating

### Update to New Version

```bash
# Pull latest docker-compose.prod.yml
git pull origin main

# Pull new images
docker-compose -f docker-compose.prod.yml pull

# Restart services (with downtime)
docker-compose -f docker-compose.prod.yml up -d

# Or rolling update (no downtime, requires orchestration)
docker-compose -f docker-compose.prod.yml up -d --no-deps --build api
```

### Rollback to Previous Version

```bash
# Edit docker-compose.prod.yml and change version tags
# From: ghcr.io/owner/orchestrator-api:v0.0.3
# To:   ghcr.io/owner/orchestrator-api:v0.0.2

# Pull old images
docker-compose -f docker-compose.prod.yml pull

# Restart
docker-compose -f docker-compose.prod.yml up -d
```

## Troubleshooting

### Services Not Starting

```bash
# Check logs
docker-compose -f docker-compose.prod.yml logs service-name

# Check health
docker-compose -f docker-compose.prod.yml ps

# Restart service
docker-compose -f docker-compose.prod.yml restart service-name
```

### Database Connection Issues

```bash
# Check PostgreSQL is healthy
docker-compose -f docker-compose.prod.yml exec postgres pg_isready

# Check connection
docker-compose -f docker-compose.prod.yml exec postgres psql -U workflow_user -d workflow_db -c "SELECT 1;"

# View PostgreSQL logs
docker-compose -f docker-compose.prod.yml logs postgres
```

### Kafka Issues

```bash
# Check Kafka logs
docker-compose -f docker-compose.prod.yml logs kafka

# List topics
docker-compose -f docker-compose.prod.yml exec kafka kafka-topics --list --bootstrap-server localhost:9092

# Check consumer groups
docker-compose -f docker-compose.prod.yml exec kafka kafka-consumer-groups --list --bootstrap-server localhost:9092
```

### Out of Memory

```bash
# Check memory usage
docker stats

# Increase limits in docker-compose.prod.yml
# Or add swap space
sudo fallocate -l 4G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

### Disk Full

```bash
# Check disk usage
df -h

# Clean up Docker
docker system prune -a

# Clean up volumes (CAUTION: deletes data)
docker volume prune
```

## Security Considerations

### Production Checklist

- [ ] Change default passwords in `.env` files
- [ ] Use secrets management (Docker secrets, Vault)
- [ ] Enable TLS for external ports (8000, 3000)
- [ ] Configure firewall rules
- [ ] Set up log rotation
- [ ] Enable authentication on Kafka (if exposed)
- [ ] Use read-only file systems where possible
- [ ] Run containers as non-root users
- [ ] Scan images for vulnerabilities
- [ ] Set up monitoring and alerts

### Recommended Changes

```yaml
# Use secrets instead of environment variables
secrets:
  db_password:
    external: true

services:
  postgres:
    environment:
      POSTGRES_PASSWORD_FILE: /run/secrets/db_password
    secrets:
      - db_password
```

## Best Practices

1. **Always use specific version tags** (not `latest`)
2. **Set resource limits** to prevent resource exhaustion
3. **Use health checks** for proper dependency management
4. **Enable restart policies** (`unless-stopped` or `always`)
5. **Monitor resource usage** regularly
6. **Back up volumes** before updates
7. **Test updates** in staging first
8. **Use logging drivers** for centralized logging
9. **Set up alerts** for service failures
10. **Document custom configurations**

## Related Documentation

- [Resource Requirements](RESOURCE_REQUIREMENTS.md)
- [Deployment Guide](DEPLOYMENT_GUIDE.md)
- [CI/CD Setup](CICD_SETUP.md)
- [Machine Sizing](../../MACHINE_SIZING_QUICK_REFERENCE.md)

---

**Auto-generated:** Yes, by GitHub Actions  
**Update Frequency:** Every push to main/master  
**Version Format:** `v{major}.{minor}.{patch}`
